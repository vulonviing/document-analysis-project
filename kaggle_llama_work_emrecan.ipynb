{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12257014,"sourceType":"datasetVersion","datasetId":7723465},{"sourceId":12295083,"sourceType":"datasetVersion","datasetId":7749306}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:36:58.985579Z","iopub.execute_input":"2025-06-26T18:36:58.985841Z","iopub.status.idle":"2025-06-26T18:36:59.276874Z","shell.execute_reply.started":"2025-06-26T18:36:58.985819Z","shell.execute_reply":"2025-06-26T18:36:59.276130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# install packages\n\n!pip install huggingface_hub\n!pip install torch \n!pip install accelerate \n!pip install transformers \n!pip install bitsandbytes\n!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:36:59.445180Z","iopub.execute_input":"2025-06-26T18:36:59.445673Z","iopub.status.idle":"2025-06-26T18:38:53.114678Z","shell.execute_reply.started":"2025-06-26T18:36:59.445642Z","shell.execute_reply":"2025-06-26T18:38:53.113987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# imports \n\nfrom transformers import pipeline\nimport os\nimport transformers\nfrom transformers import (AutoModelForCausalLM,\n                          AutoTokenizer,\n                          BitsAndBytesConfig)\nimport torch\nimport pandas as pd\nimport bitsandbytes as bnb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:38:53.116068Z","iopub.execute_input":"2025-06-26T18:38:53.116304Z","iopub.status.idle":"2025-06-26T18:39:18.346492Z","shell.execute_reply.started":"2025-06-26T18:38:53.116281Z","shell.execute_reply":"2025-06-26T18:39:18.345897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:39:18.347133Z","iopub.execute_input":"2025-06-26T18:39:18.347608Z","iopub.status.idle":"2025-06-26T18:39:18.495302Z","shell.execute_reply.started":"2025-06-26T18:39:18.347588Z","shell.execute_reply":"2025-06-26T18:39:18.494595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# empty the memory and check if the GPU is available\n\ntorch.cuda.empty_cache()\ntorch.cuda.is_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:39:38.090224Z","iopub.execute_input":"2025-06-26T18:39:38.090823Z","iopub.status.idle":"2025-06-26T18:39:38.095783Z","shell.execute_reply.started":"2025-06-26T18:39:38.090800Z","shell.execute_reply":"2025-06-26T18:39:38.095018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import data\n\ndf = pd.read_csv(\"/kaggle/input/sarcasm-detec-output/output_for_sarcasm_detection.csv\")\n\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:40:14.178351Z","iopub.execute_input":"2025-06-26T18:40:14.178893Z","iopub.status.idle":"2025-06-26T18:40:14.194810Z","shell.execute_reply.started":"2025-06-26T18:40:14.178872Z","shell.execute_reply":"2025-06-26T18:40:14.194247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sents = df[['comment','score','ups','downs','parent_comment']].values\n\nsample_text = sents[25]\nsample_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:39:40.540201Z","iopub.execute_input":"2025-06-26T18:39:40.540509Z","iopub.status.idle":"2025-06-26T18:39:40.546670Z","shell.execute_reply.started":"2025-06-26T18:39:40.540485Z","shell.execute_reply":"2025-06-26T18:39:40.546036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sents = df[['comment','score','ups','downs','parent_comment']].values\n\nsample_text = sents[0]\n\nstructured_output = {\n    'comment': sample_text[0],\n    'score': sample_text[1],\n    'ups': sample_text[2],\n    'downs': sample_text[3],\n    'parent_comment': sample_text[4]\n}\n\nstructured_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:40:37.492142Z","iopub.execute_input":"2025-06-26T18:40:37.492666Z","iopub.status.idle":"2025-06-26T18:40:37.499530Z","shell.execute_reply.started":"2025-06-26T18:40:37.492643Z","shell.execute_reply":"2025-06-26T18:40:37.498771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# specify the models name\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# add the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# quantization options to compress the model to that it fits with the memory\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, \n    bnb_4bit_quant_type = 'nf4', \n    bnb_4bit_use_double_quant = True,\n    bnb_4bit_compute_dtype = torch.bfloat16\n)\n\n# load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config = quantization_config # with quantization\n)\n\n# instantiate a pipeline\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:40:44.545252Z","iopub.execute_input":"2025-06-26T18:40:44.545983Z","iopub.status.idle":"2025-06-26T18:44:03.275242Z","shell.execute_reply.started":"2025-06-26T18:40:44.545959Z","shell.execute_reply":"2025-06-26T18:44:03.274656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# here, we specify the input consisting of a system prompt (which gived the model general instructions on how to behave)\n# some user prompts with assistant return which serve as examples\n# and the final user input with the current text to classify into sentiments.\n\nmessages = [\n        {\"role\": \"system\", \"content\": \"\"\"ead the given file. Depending on comment, parent_comment, score, ups, and downs, label whether it is sarcastic or not. Give your labels by creating a new CSV file. Columns should be: comment, parent_comment, score, ups, downs, sarcasm. Give your sarcasm decision as binary: 0 = is not, 1 = it is. For example: comment = yeah I agree LOL, parent_comment = we should decrease health expenses, score = 1, ups = 1, downs = 0, sarcasm = 1. Don't read another file, or ask me anything. Just do whatever I say. Now, here is the file:\"\"\"},\n    {\"role\": \"user\", \"content\": {\n        \"comment\": \"I hate fuckin every single person on this fuckin planet. Someone kill me pls\",\n        \"parent_comment\": \"Life is wonderful, isn't it?\",\n        \"score\": 0,\n        \"ups\": 0,\n        \"downs\": 0\n    }},\n    {\"role\": \"assistant\", \"content\": \"1\"},\n    {\"role\": \"user\", \"content\": {\n        \"comment\": \"Best day I have had in a long time :)\",\n        \"parent_comment\": \"Glad to hear things are going well!\",\n        \"score\": 0,\n        \"ups\": 0,\n        \"downs\": 0\n    }},\n    {\"role\": \"assistant\", \"content\": \"0\"},\n    {\"role\": \"user\", \"content\": {\n        \"comment\": \"boy I like called me princess Heâ€™s so precious\",\n        \"parent_comment\": \"Aw, sounds sweet!\",\n        \"score\": 0,\n        \"ups\": 0,\n        \"downs\": 0\n    }},\n    {\"role\": \"assistant\", \"content\": \"0\"},\n    {\"role\": \"user\", \"content\": {\n        \"comment\": \"Straight up in the air.\",\n        \"parent_comment\": \"I piss hard\",\n        \"score\": 1,\n        \"ups\": -1,\n        \"downs\": -1\n    }},\n    {\"role\": \"assistant\", \"content\": \"1\"},\n        {\"role\": \"user\", \"content\": structured_output},\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:44:03.282889Z","iopub.execute_input":"2025-06-26T18:44:03.283068Z","iopub.status.idle":"2025-06-26T18:44:03.302866Z","shell.execute_reply.started":"2025-06-26T18:44:03.283046Z","shell.execute_reply":"2025-06-26T18:44:03.302217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate the putput\noutputs = pipeline(\n    messages,\n    max_new_tokens=10,\n)\n\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:44:03.304289Z","iopub.execute_input":"2025-06-26T18:44:03.304539Z","iopub.status.idle":"2025-06-26T18:44:07.452683Z","shell.execute_reply.started":"2025-06-26T18:44:03.304524Z","shell.execute_reply":"2025-06-26T18:44:07.452012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:44:07.453332Z","iopub.execute_input":"2025-06-26T18:44:07.453562Z","iopub.status.idle":"2025-06-26T18:44:07.459256Z","shell.execute_reply.started":"2025-06-26T18:44:07.453544Z","shell.execute_reply":"2025-06-26T18:44:07.458408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label = outputs[0]['generated_text'][-1]['content']\nprint(f\"The predicted label for '{sample_text}' is: {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:44:34.758419Z","iopub.execute_input":"2025-06-26T18:44:34.758693Z","iopub.status.idle":"2025-06-26T18:44:34.763059Z","shell.execute_reply.started":"2025-06-26T18:44:34.758676Z","shell.execute_reply":"2025-06-26T18:44:34.762285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:46:21.537753Z","iopub.execute_input":"2025-06-26T18:46:21.538309Z","iopub.status.idle":"2025-06-26T18:46:21.548219Z","shell.execute_reply.started":"2025-06-26T18:46:21.538286Z","shell.execute_reply":"2025-06-26T18:46:21.547406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sents[0][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:47:10.240397Z","iopub.execute_input":"2025-06-26T18:47:10.240669Z","iopub.status.idle":"2025-06-26T18:47:10.245288Z","shell.execute_reply.started":"2025-06-26T18:47:10.240647Z","shell.execute_reply":"2025-06-26T18:47:10.244732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sents[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:48:15.699620Z","iopub.execute_input":"2025-06-26T18:48:15.700192Z","iopub.status.idle":"2025-06-26T18:48:15.704693Z","shell.execute_reply.started":"2025-06-26T18:48:15.700169Z","shell.execute_reply":"2025-06-26T18:48:15.704165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count = 0\nfor _, row in df.iterrows():\n    structured_output = {\n        'comment': row['comment'],\n        'score': row['score'],\n        'ups': row['ups'],\n        'downs': row['downs'],\n        'parent_comment': row['parent_comment']\n    }\n\n    count += 1\n    print(f\"{count}. {structured_output}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:38:08.036532Z","iopub.execute_input":"2025-06-26T19:38:08.036811Z","iopub.status.idle":"2025-06-26T19:38:08.084868Z","shell.execute_reply.started":"2025-06-26T19:38:08.036789Z","shell.execute_reply":"2025-06-26T19:38:08.084193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor _, row in df.iterrows():\n    structured_output = {\n        'comment': row['comment'],\n        'score': row['score'],\n        'ups': row['ups'],\n        'downs': row['downs'],\n        'parent_comment': row['parent_comment']\n    }\n\n    print(structured_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:53:16.164700Z","iopub.execute_input":"2025-06-26T18:53:16.165286Z","iopub.status.idle":"2025-06-26T18:53:16.223730Z","shell.execute_reply.started":"2025-06-26T18:53:16.165253Z","shell.execute_reply":"2025-06-26T18:53:16.223006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.iterrows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:36:21.720313Z","iopub.execute_input":"2025-06-26T19:36:21.720891Z","iopub.status.idle":"2025-06-26T19:36:21.725457Z","shell.execute_reply.started":"2025-06-26T19:36:21.720867Z","shell.execute_reply":"2025-06-26T19:36:21.724746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_emotions = []\ncounter = 0  # SayacÄ± baÅŸlat\n\nfor _, row in df.iterrows():\n    print(\"BaÅŸlÄ±yoruz...\")\n    structured_output = {\n        'comment': row['comment'],\n        'score': row['score'],\n        'ups': row['ups'],\n        'downs': row['downs'],\n        'parent_comment': row['parent_comment']\n    }\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"\"\"Read the given file. Depending on comment, parent_comment, score, ups, and downs, label whether it is sarcastic or not. Give your labels like my examples. Give your sarcasm decision as binary: 0 = is not, 1 = it is. For example: comment = yeah I agree LOL, parent_comment = we should decrease health expenses, score = 1, ups = 1, downs = 0, sarcasm = 1. Don't say anything except binary, or don't ask me anything. Just do whatever I say.:\"\"\"},\n        {\"role\": \"user\", \"content\": {\n            \"comment\": \"I hate fuckin every single person on this fuckin planet. Someone kill me pls\",\n            \"parent_comment\": \"Life is wonderful, isn't it?\",\n            \"score\": 0,\n            \"ups\": 0,\n            \"downs\": 0\n        }},\n        {\"role\": \"assistant\", \"content\": \"1\"},\n        {\"role\": \"user\", \"content\": {\n            \"comment\": \"Best day I have had in a long time :)\",\n            \"parent_comment\": \"Glad to hear things are going well!\",\n            \"score\": 0,\n            \"ups\": 0,\n            \"downs\": 0\n        }},\n        {\"role\": \"assistant\", \"content\": \"0\"},\n        {\"role\": \"user\", \"content\": {\n            \"comment\": \"boy I like called me princess Heâ€™s so precious\",\n            \"parent_comment\": \"Aw, sounds sweet!\",\n            \"score\": 0,\n            \"ups\": 0,\n            \"downs\": 0\n        }},\n        {\"role\": \"assistant\", \"content\": \"0\"},\n        {\"role\": \"user\", \"content\": {\n            \"comment\": \"Straight up in the air.\",\n            \"parent_comment\": \"I piss hard\",\n            \"score\": 1,\n            \"ups\": -1,\n            \"downs\": -1\n        }},\n        {\"role\": \"assistant\", \"content\": \"1\"},\n        {\"role\": \"user\", \"content\": structured_output},\n    ]\n\n    output = pipeline(messages, max_new_tokens=10)\n    counter += 1\n    print(f\"Ä°ÅŸlem: {counter}/{len(df)}\")\n\n    generated_text = output[0][\"generated_text\"]\n    for message in reversed(generated_text):\n        if message[\"role\"] == \"assistant\":\n            label = message[\"content\"]\n            break\n\n    predicted_emotions.append(label)\n\ndf[\"predicted_emotion\"] = predicted_emotions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:40:16.526664Z","iopub.execute_input":"2025-06-26T19:40:16.526941Z","iopub.status.idle":"2025-06-26T20:21:25.459333Z","shell.execute_reply.started":"2025-06-26T19:40:16.526921Z","shell.execute_reply":"2025-06-26T20:21:25.458590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_emotions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T20:25:19.442714Z","iopub.execute_input":"2025-06-26T20:25:19.443421Z","iopub.status.idle":"2025-06-26T20:25:19.451418Z","shell.execute_reply.started":"2025-06-26T20:25:19.443397Z","shell.execute_reply":"2025-06-26T20:25:19.450712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['sarcasm'] = predicted_emotions\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T20:27:25.234489Z","iopub.execute_input":"2025-06-26T20:27:25.234756Z","iopub.status.idle":"2025-06-26T20:27:25.244971Z","shell.execute_reply.started":"2025-06-26T20:27:25.234736Z","shell.execute_reply":"2025-06-26T20:27:25.244222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/sarcasm_labeled_LLama3-1-8B-Instruct.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T20:28:07.921119Z","iopub.execute_input":"2025-06-26T20:28:07.921675Z","iopub.status.idle":"2025-06-26T20:28:07.936719Z","shell.execute_reply.started":"2025-06-26T20:28:07.921653Z","shell.execute_reply":"2025-06-26T20:28:07.936023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}